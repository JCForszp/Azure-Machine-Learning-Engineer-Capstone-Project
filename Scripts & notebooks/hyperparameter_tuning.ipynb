{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning using HyperDrive\n",
    "\n",
    "TODO: Import Dependencies. In the cell below, import all the dependencies that you will need to complete the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598531914256
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import pkg_resources\n",
    "\n",
    "from azureml.widgets import RunDetails                                        # needed to access the RunDetails widget in section \"Run Details\", below. \n",
    "from azureml.train.sklearn import SKLearn\n",
    "from azureml.train.hyperdrive.run import PrimaryMetricGoal\n",
    "from azureml.train.hyperdrive.policy import BanditPolicy\n",
    "from azureml.train.hyperdrive.sampling import RandomParameterSampling\n",
    "from azureml.train.hyperdrive.runconfig import HyperDriveConfig\n",
    "from azureml.train.hyperdrive.parameter_expressions import choice, uniform\n",
    "import os\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'heart-failure-experiment-hyperd'\n",
    "project_folder = './capstone-project'\n",
    "\n",
    "experiment = Experiment(ws, experiment_name)\n",
    "experiment\n",
    "\n",
    "# starting an interactive logging session, as recommended in Azure documentation 'how-to-log-view-metrics'\n",
    "run=experiment.start_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create compute cluster and choose a name for it\n",
    "cpu_cluster_name = \"compute-cluster\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute cluster...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='Standard_DS3_v2', min_nodes=1, max_nodes=4)\n",
    "    compute_target = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
    "\n",
    "# Can poll for a minimum number of nodes and for a specific timeout. \n",
    "# If no min node count is provided it uses the scale settings for the cluster.\n",
    "compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "# use get_status() to get a detailed status for the current cluster. \n",
    "print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "### Overview\n",
    "\n",
    "I will use [Kaggle](https://www.kaggle.com/andrewmvd/heart-failure-clinical-data) \"Heart Failure Prediction dataset\".  \n",
    "This dataset is related to a study that  focused on survival analysis of 299 heart failure patients who were admitted to Institute   \n",
    "of Cardiology and Allied hospital Faisalabad-Pakistan during April-December (2015).   \n",
    "All the patients were aged 40 years or above, having left ventricular systolic dysfunction.  \n",
    "\n",
    "The dataset contains the following 12 clinical features, plus one target feature (\"death event\"):  \n",
    "A data analysis report is available onmy github repo, [here](https://github.com/JCForszp/nd00333-capstone/blob/master/Datasets/heart%20failure%20report.html)\n",
    "\n",
    "**Clinical features:**\n",
    "- age: age of the patient (years)\n",
    "- anaemia: decrease of red blood cells or hemoglobin (boolean)\n",
    "- high blood pressure: if the patient has hypertension (boolean)\n",
    "- creatinine phosphokinase (CPK): level of the CPK enzyme in the blood (mcg/L)\n",
    "- diabetes: if the patient has diabetes (boolean)\n",
    "- ejection fraction: percentage of blood leaving the heart at each contraction (percentage)\n",
    "- platelets: platelets in the blood (kiloplatelets/mL)\n",
    "- sex: woman or man (binary)\n",
    "- serum creatinine: level of serum creatinine in the blood (mg/dL)\n",
    "- serum sodium: level of serum sodium in the blood (mEq/L)\n",
    "- smoking: if the patient smokes or not (boolean)\n",
    "- time: follow-up period (days)\n",
    "\n",
    "**Target feature:**\n",
    "- [target] death event: if the patient deceased during the follow-up period (boolean)\n",
    "\n",
    "We are dealing here with a classification task, i.e trying to predict the outcome of the follow-up period based on the given clinical features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1598531917374
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "found = False\n",
    "key = \"JCF-heart-failure-dataset\"\n",
    "description_text = \"Kaggle Heart Failure Prediction dataset\"\n",
    "\n",
    "if key in ws.datasets.keys(): \n",
    "        found = True\n",
    "        dataset = ws.datasets[key] \n",
    "\n",
    "if not found:\n",
    "        # Create AML Dataset and register it into Workspace\n",
    "        example_data = 'https://github.com/JCForszp/nd00333-capstone/blob/master/Datasets/heart_failure_clinical_records_dataset.csv'\n",
    "        dataset = Dataset.Tabular.from_delimited_files(example_data)        \n",
    "        #Register Dataset in Workspace\n",
    "        dataset = dataset.register(workspace=ws,\n",
    "                                   name=key,\n",
    "                                   description=description_text)\n",
    "\n",
    "\n",
    "df = dataset.to_pandas_dataframe()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gather": {
     "logged": 1598531923519
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Hyperdrive Configuration\n",
    "\n",
    "For this part of the project, I chose a Logistic Regression model that fits well binary classification problems.  \n",
    "Features are weakly correlated and the presence of categorical variables (anemia, diabetes, sex) is supported by this algorithm.  \n",
    "So, LR seems well suited. \n",
    "\n",
    "The aim here is to fine-tune the model hyper-parameters using Azure HyperDrive. \n",
    "HyperDrive configuration will be split below into 3 sections:\n",
    "1. early termination policy\n",
    "2. creation of the estimator and of the different parameters that will be used during the training\n",
    "3. the hyper drive configuration run in itself\n",
    "\n",
    "### 1. early_termination_policy\n",
    "> Regarding early termination of poorly performing runs, I used the BanditPolicy.  \n",
    "> The BanditPolicy defines a slack factor (defined here to 0.1).  \n",
    "> All runs that fall outside the slack factor with respect to the best performing run will be terminated, saving time and budget.\n",
    "\n",
    "### 2. estimator and parameters sampling\n",
    "**estimator**\n",
    "> We will use a Logistic Regression and use 'accuracy'.  \n",
    "> AuC would also have been an option.  \n",
    "\n",
    "**Hyperparameter space**  \n",
    "> I chose the RandomParameterSampling, mainly for speed reason, as the usual alternative, GridParameterSampling, would have triggered   \n",
    "> an exhaustive search over the complete space, for a gain that proved to be relatively small at the end.  \n",
    "> Also, GridParameterSampling only allows discrete values, while random sampling is more open as it allows also the use of continuous values.  \n",
    "> Finally, RandomParameterSampling supports early termination of low-performance runs.  \n",
    "> For those three reasons, and within the given context of this analysis, RandomParameterSampling appeared as the best option.  \n",
    "\n",
    "### 3. HyperDrive run configuration  \n",
    "> This configuration object aggregates the settings defined for the policy, the choice of estimator and the hyper-parameters space definition.  \n",
    "> We define here also the primary metric used ('accuracy') that we want to maximize (**primary_metric_goal**).  \n",
    "> **max_total_runs** sets a limit of the maximum number of runs that can be created. We set it here to 100.  \n",
    "> **max_concurrent_runs** is aligned to the compute target available resources available (4).  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1598544893076
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Create an early termination policy. This is not required if you are using Bayesian sampling.\n",
    "early_termination_policy = BanditPolicy(slack_factor=0.1, evaluation_interval=1)\n",
    "\n",
    "#TODO: Create the different params that you will be using during training\n",
    "param_sampling = = RandomParameterSampling(\n",
    "                        {\n",
    "                            \"--C\": uniform(0.01 , 1.99),\n",
    "                            \"--max_iter\": choice(range(50,150,10))\n",
    "                        }\n",
    "                        )\n",
    "\n",
    "#TODO: Create your estimator and hyperdrive config\n",
    "estimator = SKLearn(source_directory=os.path.join(\"./\"), compute_target=compute_cluster_target,entry_script=\"train.py\")\n",
    "\n",
    "hyperdrive_run_config = HyperDriveConfig(estimator=estimator, \n",
    "                                    hyperparameter_sampling=param_sampling, \n",
    "                                    policy=early_termination_policy,\n",
    "                                    primary_metric_name=\"Accuracy\",\n",
    "                                    primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n",
    "                                    max_total_runs=100,\n",
    "                                    max_concurrent_runs=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1598544897941
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#TODO: Submit your experiment\n",
    "hyperdrive_run = experiment.submit(config=hyperdrive_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gather": {
     "logged": 1598544898497
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Run Details\n",
    "\n",
    "OPTIONAL: Write about the different models trained and their performance. Why do you think some models did better than others?\n",
    "\n",
    "TODO: In the cell below, use the `RunDetails` widget to show the different experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1598546648408
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "RunDetails(hyperdrive_run).show()\n",
    "hyperdrive_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdr.wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model\n",
    "\n",
    "TODO: In the cell below, get the best model from the hyperdrive experiments and display all the properties of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1598546650307
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "best_run=hyperdrive_run.get_best_run_by_primary_metric()\n",
    "best_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best run metrics: {best_run.get_metrics()}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1598546657829
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#TODO: Save the best model\n",
    "best_run.register_model(model_name = \"best_run_hyperdrive.pkl\", model_path = './outputs/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Model Deployment\n",
    "\n",
    "Remember you have to deploy only one of the two models you trained.. Perform the steps in the rest of this notebook only if you wish to deploy this model.\n",
    "\n",
    "TODO: In the cell below, register the model, create an inference config and deploy the model as a web service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: In the cell below, send a request to the web service you deployed to test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: In the cell below, print the logs of the web service and delete the service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
