{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated ML  \n",
    "\n",
    "> All the dependencies needed to complete the project are listed below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1598423888013
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import csv\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import pkg_resourceses\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.core.workspace import Workspace\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "from azureml.core.dataset import Dataset\n",
    "from azureml.data.dataset_factory import TabularDatasetFactory\n",
    "\n",
    "# Dependencies required to create or attach AmlCompute cluster:\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "from azureml.pipeline.steps import AutoMLStep\n",
    "\n",
    "# needed to display the run details\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "import joblib\n",
    "# Needed for the deployment part\n",
    "from azureml.core.environment import Environment \n",
    "from azureml.core.model import InferenceConfig \n",
    "from azureml.core.webservice import AciWebservice, Webservice\n",
    "from azureml.core.model import Model\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> I will bring at this stage the creation of the experiment and the creation / attachment of the AmlCompute to the workspace.  \n",
    "> Doing so, I will keep the same approach that was suggested during the second Udacity project. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'heart-failure-experiment-automl'\n",
    "project_folder = './capstone-project'\n",
    "\n",
    "experiment = Experiment(ws, experiment_name)\n",
    "experiment\n",
    "\n",
    "# starting an interactive logging session, as recommended in Azure documentation 'how-to-log-view-metrics'\n",
    "run=experiment.start_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: update the cluster name to match the existing cluster\n",
    "# Choose a name for your CPU cluster\n",
    "amlcompute_cluster_name = \"aml-compute-cluster\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=amlcompute_cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='Standard_DS3_v2',# for GPU, use \"STANDARD_NC6\"\n",
    "                                                           #vm_priority = 'lowpriority', # optional\n",
    "                                                           min_nodes=1,\n",
    "                                                           max_nodes=4)\n",
    "    compute_target = ComputeTarget.create(ws, amlcompute_cluster_name, compute_config)\n",
    "\n",
    "compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "# I am using here the get_status() for a more detailed view of current AmlCompute status:\n",
    "print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "### Overview\n",
    "\n",
    "I will use [Kaggle](https://www.kaggle.com/andrewmvd/heart-failure-clinical-data) \"Heart Failure Prediction dataset\".  \n",
    "This dataset is related to a study that  focused on survival analysis of 299 heart failure patients who were admitted to Institute   \n",
    "of Cardiology and Allied hospital Faisalabad-Pakistan during April-December (2015).   \n",
    "All the patients were aged 40 years or above, having left ventricular systolic dysfunction.  \n",
    "\n",
    "The dataset contains the following 12 clinical features, plus one target feature (\"death event\"):  \n",
    "A data analysis report is available onmy github repo, [here](https://github.com/JCForszp/nd00333-capstone/blob/master/Datasets/heart%20failure%20report.html)\n",
    "\n",
    "**Clinical features:**\n",
    "- age: age of the patient (years)\n",
    "- anaemia: decrease of red blood cells or hemoglobin (boolean)\n",
    "- high blood pressure: if the patient has hypertension (boolean)\n",
    "- creatinine phosphokinase (CPK): level of the CPK enzyme in the blood (mcg/L)\n",
    "- diabetes: if the patient has diabetes (boolean)\n",
    "- ejection fraction: percentage of blood leaving the heart at each contraction (percentage)\n",
    "- platelets: platelets in the blood (kiloplatelets/mL)\n",
    "- sex: woman or man (binary)\n",
    "- serum creatinine: level of serum creatinine in the blood (mg/dL)\n",
    "- serum sodium: level of serum sodium in the blood (mEq/L)\n",
    "- smoking: if the patient smokes or not (boolean)\n",
    "- time: follow-up period (days)\n",
    "\n",
    "**Target feature:**\n",
    "- [target] death event: if the patient deceased during the follow-up period (boolean)\n",
    "\n",
    "We are dealing here with a classification task, i.e trying to predict the outcome of the follow-up period based on the given clinical features.\n",
    "\n",
    "\n",
    "TODO: Get data. In the cell below, write code to access the data you will be using in this project. Remember that the dataset needs to be external."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1598423890461
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "found = False\n",
    "key = \"JCF-heart-failure-dataset\"\n",
    "description_text = \"Kaggle Heart Failure Prediction dataset\"\n",
    "\n",
    "if key in ws.datasets.keys(): \n",
    "        found = True\n",
    "        dataset = ws.datasets[key] \n",
    "\n",
    "if not found:\n",
    "        # Create AML Dataset and register it into Workspace\n",
    "        example_data = 'https://github.com/JCForszp/nd00333-capstone/blob/master/Datasets/heart_failure_clinical_records_dataset.csv'\n",
    "        dataset = Dataset.Tabular.from_delimited_files(example_data)        \n",
    "        #Register Dataset in Workspace\n",
    "        dataset = dataset.register(workspace=ws,\n",
    "                                   name=key,\n",
    "                                   description=description_text)\n",
    "\n",
    "\n",
    "df = dataset.to_pandas_dataframe()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy obtained by predicting the most frequent value (\"null acccuracy\", as baseline) : {df['DEATH_EVENT'].value_counts().head(1)/len(df['DEATH_EVENT'])}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoML Configuration\n",
    "\n",
    "### Note on automl settings selection:  \n",
    "\n",
    "- **n_cross_validations**: 4,\n",
    "10 is a usual value for cross-validations, but the size of the dataset is relatively small.  \n",
    "Hence, a 90/10% split seems a bit disproportionate. I prefer to take a 75/25% split,  \n",
    "which will end of with testing sets of 75 patients, so probably more reasonable and keeping the  \n",
    "number of runs low. \n",
    "- **primary_metric**: 'accuracy',  \n",
    "'accuracy' is the most frequent and easiest metrics to use for classification tasks  \n",
    "- **enable_early_stopping**: True,  \n",
    "According to [Azure documentation](https://docs.microsoft.com/en-us/python/api/azureml-train-automl-client/azureml.train.automl.automlconfig.automlconfig?view=azure-ml-py), this  \n",
    "settings allows automl to terminate a score determination if the score is not improving.  \n",
    "The default value is 'False' and hence, needs to be set to 'True' at config level.  \n",
    "Microsoft documentation mentions that *Early stopping window starts on the 21st iteration  \n",
    "and looks for early_stopping_n_iters iterations (currently set to 10).  \n",
    "This means that the first iteration where stopping can occur is the 31st.*    \n",
    "Hence, this setting is a nice to have, but won't be critical for our limited exercice. \n",
    "- **max_concurrent_iterations**: 4,  \n",
    "According to Microsoft documentation *Represents the maximum number of iterations that would be executed in parallel.  \n",
    "The default value is 1.*  \n",
    "In our compute_config, we chose a value of 4, and the number of concurrent values needs to be less or equal to that number.  \n",
    "Hence, the value of this setting. \n",
    "- **experiment_timeout_minutes**: 20,    \n",
    "Defines how long, in minutes, the experiment should continue to run. \n",
    "Looking at Azure documentation on [how-to-configure-auto-train](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-configure-auto-train),  \n",
    "20mn seemed to be a reasonable trade-off.  \n",
    "- **verbosity**: logging.INFO  \n",
    "The verbosity level for writing to the log file. The default is INFO or 20. So, we could basically have skipped this setting, but it seems  \n",
    "good practice to specify it every time, to assess if it's really the optimal level of detail.\n",
    "\n",
    "### Note on automl_config settings:\n",
    "- **compute_target** = compute_target,  \n",
    "This is the Azure Machine Learning compute target to run the Automated Machine Learning experiment on.   \n",
    "It corresponds to the compute_target we defined above in the script, right after the import of the dependencies.  \n",
    "- **task**='classification',  \n",
    "Three types of tasks are allowed here:'classification', 'regression', or 'forecasting'.  \n",
    "As mentioned in the Dataset section, we are clearly here in a classification task. \n",
    "- **training_data**=dataset,  \n",
    "This is the dataset we registered in previous cell. \n",
    "- **label_column_name**='DEATH_EVENT',  \n",
    "This is the name of the target column. The original dataset on Kaggle clearly defines 'DEATH_EVENT' as being the label column.  \n",
    "- **path** = project_folder,  \n",
    "We set this project_folder to './capstone-project'  \n",
    "- **featurization**= 'auto',  \n",
    "Two values allowed: 'auto' and 'off'. Based on Microsoft doc, setting featurization to off  would mean re-doing manually    \n",
    "all one-hot encoding, managing missing values,... It meakes total sense to leave automl dealing with that on a pre-cleaned dataset. \n",
    "- **debug_log** = \"automl_errors.log\",  \n",
    "The log file to write debug information to. If not specified, 'automl.log' is used.  \n",
    "I just set the name to one I chose.  \n",
    "- **enable_onnx_compatible_models**=False,  \n",
    "ONNX is presented as a way to optimize the inference of the ML model.[(doc)](https://docs.microsoft.com/en-us/azure/machine-learning/concept-onnx)   \n",
    "We are dealing with a small-sized dataset, so I chose to leave this setting of False and I will investigate this feature separately later.   \n",
    "- **automl_settings**  \n",
    "Brings the automl_settings dictionary we defined above in the automl_config object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1598429217746
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Put your automl settings here\n",
    "automl_settings = {\"n_cross_validations\": 5,\n",
    "                    \"primary_metric\": 'accuracy',\n",
    "                    \"enable_early_stopping\": True,\n",
    "                    \"max_concurrent_iterations\": 4,\n",
    "                    \"experiment_timeout_minutes\": 20,\n",
    "                    \"verbosity\": logging.INFO\n",
    "                    }\n",
    "\n",
    "# TODO: Put your automl config here\n",
    "automl_config = AutoMLConfig(\n",
    "                            compute_target = compute_target,\n",
    "                            task='classification',\n",
    "                            training_data=dataset,\n",
    "                            label_column_name='DEATH_EVENT',\n",
    "                            path = project_folder,\n",
    "                            featurization= 'auto',\n",
    "                            debug_log = \"automl_errors.log\",\n",
    "                            enable_onnx_compatible_models=False,\n",
    "                            **automl_settings\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1598431107951
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Experiment Submission\n",
    "# Theshow_output parameter switches on the verbose logging\n",
    "my_run = experiment.submit(automl_config, show_output = True)\n",
    "# We use the same parameter in the wait_for_completion function on the resulting run.\n",
    "my_run.wait_for_completion(show_output = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This command fetches the run status and displays it in this notebook as confirmation. \n",
    "print(\"Run Status: \",my_run.get_status())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Details\n",
    "\n",
    ">We  use here the `RunDetails` widget to show the different experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431121770
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "RunDetails(my_run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_run.wait_for_completion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AutoML Run Summary: \", my_run.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model\n",
    "\n",
    "In the cell below, we get the best model from the automl experiments and display all the properties of the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval of best model from the automl experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1598431425670
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "best_run, fitted_model = my_run.get_output() # Return the run with the corresponding best pipeline that has already been tested.\n",
    "                                             # as we do not mention any parameter, get_output returns the best pipeline according to the primary metric ('accuracy'.\n",
    "best_run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display of all the properties of the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431425670
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# we display, below, the metrics, details and properties (this is the order that makes most sense)\n",
    "\n",
    "print('*'*50)\n",
    "best_run_metrics = best_run.get_metrics()\n",
    "for metric_name in best_run_metrics:\n",
    "    metric = best_run_metrics[metric_name]\n",
    "    print(metric_name,\":\" , metric)\n",
    "\n",
    "print('*'*50)\n",
    "print(\"Best run details :\",best_run.get_details())\n",
    "\n",
    "print('*'*50)\n",
    "print(\"Best run properties :\",best_run.get_properties())\n",
    "print('*'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fitted_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gather": {
     "logged": 1598431426111
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Saving the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('outputs', exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = best_run.register_model(\n",
    "                                workspace=ws,\n",
    "                                model_name = 'best_run', \n",
    "                                model_path = './outputs/model.pckl',\n",
    "                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment\n",
    "\n",
    "Remember you have to deploy only one of the two models you trained.. Perform the steps in the rest of this notebook only if you wish to deploy this model.\n",
    "\n",
    "TODO: In the cell below, register the model, create an inference config and deploy the model as a web service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1598431435189
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "best.run.get_file_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1598431435189
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "best_run.download_file('outputs/scoring_file_v_1_0_0.py','score.py')\n",
    "# automl_best_run.download_file('outputs/conda_env_v_1_0_0.yml','env.yml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gather": {
     "logged": 1598431657736
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "TODO: In the cell below, send a request to the web service you deployed to test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1598432707604
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Creating an inference config\n",
    "inference_config = InferenceConfig(\n",
    "                                    entry_script=\"score.py\",\n",
    "                                    environment=best_run.get_environment()\n",
    "                                  )\n",
    "\n",
    "# Deploying the model as a web service to an Azure Container Instance (ACI)\n",
    "aci_config = AciWebservice.deploy_configuration(cpu_cores=1, memory_gb=1)\n",
    "\n",
    "service_name = 'heartfprediction'\n",
    "webservice = Model.deploy(workspace=ws,\n",
    "                       name=service_name,\n",
    "                       models=[model],\n",
    "                       inference_config=inference_config,\n",
    "                       deployment_config=aci_config)\n",
    "\n",
    "webservice.wait_for_deployment(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(webservice.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "primary_key, secundary_key = webservice.get_keys()\n",
    "print(\"Service State: \", webservice.state)\n",
    "print(\"Scoring URI: \",   webservice.scoring_uri)\n",
    "print(\"Swagger URI: \",   webservice.swagger_uri)\n",
    "print(\"Primary key: \",primary_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\"DEATH_EVENT\"])\n",
    "\n",
    "input_data = json.dumps({\n",
    "                        'data': df.sample(10).to_dict(orient='records')\n",
    "                        })\n",
    "\n",
    "# I sent a random sample and expect a proportion of negative death event ('0') between 5 and 7 or 8,   \n",
    "# based on the dataset proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_uri = webservice.scoring_uri\n",
    "input_data = data_sample\n",
    "\n",
    "# Set the content type\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "\n",
    "# Make the request and display the response\n",
    "response = requests.post(scoring_uri, input_data, headers=headers)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for case in range(0,10):\n",
    "    print(f\"Case details: {data['data'][case]}\")\n",
    "    print(f\"Death event: {'yes' if response.json()[case]==1 else 'no'}\")\n",
    "    print('*'*20)\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gather": {
     "logged": 1598432765711
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "TODO: In the cell below, print the logs of the web service and delete the service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "webservice.get_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webservice.delete()"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
